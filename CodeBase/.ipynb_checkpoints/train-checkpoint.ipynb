{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "determined-township",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import argparse\n",
    "import torch\n",
    "from model import YNet,Transformer\n",
    "from easydict import EasyDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "finished-potential",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e567fbe0",
   "metadata": {},
   "source": [
    "#### Load config file and print hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "arabic-thickness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'resize': 0.25,\n",
       " 'batch_size': 4,\n",
       " 'viz_epoch': 10,\n",
       " 'learning_rate': 0.0001,\n",
       " 'num_epochs': 300,\n",
       " 'waypoints': [11],\n",
       " 'temperature': 1.0,\n",
       " 'segmentation_model_fp': 'segmentation_models/SDD_segmentation.pth',\n",
       " 'semantic_classes': 6,\n",
       " 'loss_scale': 1000,\n",
       " 'kernlen': 31,\n",
       " 'nsig': 4,\n",
       " 'use_features_only': False,\n",
       " 'unfreeze': 150,\n",
       " 'use_TTST': True,\n",
       " 'rel_threshold': 0.01,\n",
       " 'use_CWS': False,\n",
       " 'CWS_params': 'None',\n",
       " 'dataset_name': 'sdd',\n",
       " 'train_data_path': '../dataset/SDD/train_trajnet.pkl',\n",
       " 'train_image_path': '../dataset/SDD/train',\n",
       " 'val_data_path': '../dataset/SDD/test_trajnet.pkl',\n",
       " 'val_image_path': '../dataset/SDD/test',\n",
       " 'num_goals': 20,\n",
       " 'num_traj': 1,\n",
       " 'experiment_name': 'sdd_trajnet',\n",
       " 'model': {'name': 'ynet',\n",
       "  'kwargs': {'obs_len': 8,\n",
       "   'pred_len': 12,\n",
       "   'segmentation_model_fp': 'segmentation_models/inD_segmentation.pth',\n",
       "   'use_features_only': False,\n",
       "   'semantic_classes': 6,\n",
       "   'encoder_channels': [32, 32, 64, 64, 64],\n",
       "   'decoder_channels': [64, 64, 64, 32, 32],\n",
       "   'waypoints': [11]}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict = {\n",
    "    'ynet': YNet,\n",
    "    'transformer': Transformer,\n",
    "}\n",
    "CONFIG_FILE_PATH = 'config/sdd_trajnet.yaml'  # yaml config file containing all the hyperparameters\n",
    "with open(CONFIG_FILE_PATH) as file:\n",
    "    params = yaml.load(file, Loader=yaml.FullLoader)\n",
    "params = EasyDict(params)\n",
    "experiment_name = CONFIG_FILE_PATH.split('.yaml')[0].split('config/')[1]\n",
    "params\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f729e8f",
   "metadata": {},
   "source": [
    "#### Some hyperparameters and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dangerous-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = params.experiment_name  # arbitrary name for this experiment\n",
    "DATASET_NAME = params.dataset_name\n",
    "\n",
    "TRAIN_DATA_PATH = params.train_data_path\n",
    "TRAIN_IMAGE_PATH = params.train_image_path\n",
    "VAL_DATA_PATH = params.val_data_path\n",
    "VAL_IMAGE_PATH = params.val_image_path\n",
    "# OBS_LEN = params.obs_length  # in timesteps\n",
    "# PRED_LEN = params.pred_len  # in timesteps\n",
    "NUM_GOALS = params.num_goals  # K_e\n",
    "NUM_TRAJ = params.num_traj  # K_a\n",
    "\n",
    "BATCH_SIZE = params.batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-pressure",
   "metadata": {},
   "source": [
    "#### Load preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "german-feature",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle(TRAIN_DATA_PATH)\n",
    "df_val = pd.read_pickle(VAL_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "corporate-pharmacy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>trackId</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>sceneId</th>\n",
       "      <th>metaId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>757.0</td>\n",
       "      <td>918.5</td>\n",
       "      <td>bookstore_0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>765.0</td>\n",
       "      <td>918.5</td>\n",
       "      <td>bookstore_0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>100</td>\n",
       "      <td>773.0</td>\n",
       "      <td>918.5</td>\n",
       "      <td>bookstore_0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36</td>\n",
       "      <td>100</td>\n",
       "      <td>780.5</td>\n",
       "      <td>918.5</td>\n",
       "      <td>bookstore_0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48</td>\n",
       "      <td>100</td>\n",
       "      <td>788.5</td>\n",
       "      <td>919.5</td>\n",
       "      <td>bookstore_0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   frame  trackId      x      y      sceneId  metaId\n",
       "0      0      100  757.0  918.5  bookstore_0       0\n",
       "1     12      100  765.0  918.5  bookstore_0       0\n",
       "2     24      100  773.0  918.5  bookstore_0       0\n",
       "3     36      100  780.5  918.5  bookstore_0       0\n",
       "4     48      100  788.5  919.5  bookstore_0       0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dc5d7c",
   "metadata": {},
   "source": [
    "#### Initiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "harmful-colleague",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvidia-3080/anaconda3/envs/hxy/lib/python3.9/site-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'segmentation_models_pytorch.unet.model.Unet' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/nvidia-3080/anaconda3/envs/hxy/lib/python3.9/site-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'segmentation_models_pytorch.encoders.resnet.ResNetEncoder' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/nvidia-3080/anaconda3/envs/hxy/lib/python3.9/site-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/nvidia-3080/anaconda3/envs/hxy/lib/python3.9/site-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/nvidia-3080/anaconda3/envs/hxy/lib/python3.9/site-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/nvidia-3080/anaconda3/envs/hxy/lib/python3.9/site-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.MaxPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/nvidia-3080/anaconda3/envs/hxy/lib/python3.9/site-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/nvidia-3080/anaconda3/envs/hxy/lib/python3.9/site-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torchvision.models.resnet.Bottleneck' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/nvidia-3080/anaconda3/envs/hxy/lib/python3.9/site-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Identity' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/nvidia-3080/anaconda3/envs/hxy/lib/python3.9/site-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.container.ModuleList' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/nvidia-3080/anaconda3/envs/hxy/lib/python3.9/site-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'segmentation_models_pytorch.base.modules.Activation' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/nvidia-3080/anaconda3/envs/hxy/lib/python3.9/site-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.activation.Softmax' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "model = model_dict[params.model.name](**params.model.kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e099fe",
   "metadata": {},
   "source": [
    "#### Start training\n",
    "Note, the Val ADE and FDE are without TTST and CWS to save time. Therefore, the numbers will be worse than the final values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "optional-colleague",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prepare Dataset: 100%|█████████████████████| 240/240 [00:00<00:00, 1180.33it/s]\n",
      "/media/nvidia-3080/35aae167-c57e-42a4-aae3-7dbbf1275d98/hxy/TrajectoryPredict/CodeBase/utils/dataloader.py:35: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(trajectories), meta, scene_list\n",
      "Prepare Dataset: 100%|███████████████████████| 17/17 [00:00<00:00, 1408.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|                                           | 0/300 [00:00<?, ?it/s]/media/nvidia-3080/35aae167-c57e-42a4-aae3-7dbbf1275d98/hxy/TrajectoryPredict/CodeBase/utils/dataloader.py:46: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272204863/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  return torch.Tensor(trajectories).squeeze(0), meta, scene[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvidia-3080/anaconda3/envs/hxy/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272204863/work/aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([3, 8, 2]) gt:torch.Size([3, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                           | 0/300 [00:05<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n",
      "traj:torch.Size([4, 8, 2]) gt:torch.Size([4, 12, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3028577/3438813135.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.train(df_train, df_val, params, train_image_path=TRAIN_IMAGE_PATH, val_image_path=VAL_IMAGE_PATH, \n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0mexperiment_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEXPERIMENT_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_goals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_GOALS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_traj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_TRAJ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             device=None, dataset_name=DATASET_NAME)\n",
      "\u001b[0;32m/media/nvidia-3080/35aae167-c57e-42a4-aae3-7dbbf1275d98/hxy/TrajectoryPredict/CodeBase/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_data, val_data, params, train_image_path, val_image_path, experiment_name, batch_size, num_goals, num_traj, device, dataset_name)\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Start training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m                         train_ADE, train_FDE, train_loss = train(model, train_loader, train_images, e, obs_len, pred_len,\n\u001b[0m\u001b[1;32m    314\u001b[0m                                                                                                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_template\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                                                                                                          input_template, optimizer, criterion, dataset_name, self.homo_mat)\n",
      "\u001b[0;32m/media/nvidia-3080/35aae167-c57e-42a4-aae3-7dbbf1275d98/hxy/TrajectoryPredict/CodeBase/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, train_images, e, obs_len, pred_len, batch_size, params, gt_template, device, input_template, optimizer, criterion, dataset_name, homo_mat)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                         \u001b[0mgt_future\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrajectory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                         \u001b[0mgt_future_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_patch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_template\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                         \u001b[0mgt_future_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_future_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"traj:{} gt:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mobs_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/nvidia-3080/35aae167-c57e-42a4-aae3-7dbbf1275d98/hxy/TrajectoryPredict/CodeBase/utils/image_utils.py\u001b[0m in \u001b[0;36mget_patch\u001b[0;34m(template, traj, H, W)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0my_up\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mpatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_l\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_l\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx_u\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_u\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_low\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_up\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_low\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_up\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/nvidia-3080/35aae167-c57e-42a4-aae3-7dbbf1275d98/hxy/TrajectoryPredict/CodeBase/utils/image_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0my_up\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mpatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_l\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_l\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx_u\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_u\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_low\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_up\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_low\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_up\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(df_train, df_val, params, train_image_path=TRAIN_IMAGE_PATH, val_image_path=VAL_IMAGE_PATH, \n",
    "            experiment_name=EXPERIMENT_NAME, batch_size=BATCH_SIZE, num_goals=NUM_GOALS, num_traj=NUM_TRAJ, \n",
    "            device=None, dataset_name=DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fe307e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
